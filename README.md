lydia-radgiver-frontend
================

Frontend for IA r√•dgivere

# Komme i gang

## Kj√∏re opp lokalt
Dette let deg k√∏yre opp frontend lokalt med alt av avhengigheitene som trengst. 
Dette vert orkestrert av docker-compose. 


### Avhengigheiter som vert k√∏yrd
- backend: lydia-api
- postgres
- kafka
- wonderwall
- mockOAuth2-server


### F√∏r du startar
Sjekk om du har postgresql-klient ved √• k√∏yre
`psql --version`

Installer ein postgresql-klient, for eksempel ved √• gjere  
`brew install libpq`  
Legg s√• til psql i PATH. Du vil f√• instruksjonar p√• dette fr√• brew, og det ser litt s√•nn ut:  
> If you need to have libpq first in your PATH, run:  
> echo 'export PATH="/usr/local/opt/libpq/bin:$PATH"' >> ~/.zshrc

Sjekk at du har colima `colima version`  
Om du ikkje har colima, k√∏yr `brew install colima`

Sjekk at du har docker `docker -v`  
Om du ikkje har docker, k√∏yr `brew install docker`


Vi skal leggje til linja `127.0.0.1 host.docker.internal` i fila `/etc/hosts` i rota av datamaskina di.
Da vil nettlesaren automatisk fange opp wonderwall og mock-oauth2-server, som da vil resolve det til localhost.

Dette kan du til d√∏mes gjere ved √• skrive `sudo vi /etc/hosts` i terminalen og s√• 
leggje til `127.0.0.1 host.docker.internal` nedst p√• ei ega linje.
S√∏rg for at du har skrudd p√• administrator-rettar for brukaren din (privileges.app p√• mac).



### K√∏yr server og frontend med avhengigheter:  
Pass p√• at du har ein k√∏yrande docker-instans. 
For Colima gjer du dette ved √• k√∏yre `colima status` og eventuelt `colima start`

Om du bruker Colima treng du ogs√• √• k√∏yre denne for √• f√• ting til √• fungere. Hugs √• gjer brukaren din admin-rettar f√∏rst.  
`sudo rm -rf /var/run/docker.sock && sudo ln -s /Users/$(whoami)/.colima/docker.sock /var/run/docker.sock
`

N√•r du har docker p√• plass kan du k√∏yre   
`./run.sh -cif` i rotmappa. Dette startar server og frontend med sine avhengigheiter.

Bes√∏k deretter http://localhost:2222 i din favorittbrowser.  
Skjemaet som dukker opp her gjev deg h√∏ve til √• endre kva rettar testbrukaren din har. I dei fleste tilfelle kan du skrive inn noko tilfeldig tekst og trykke "SIGN-IN".

### Rydd opp etter deg n√•r du er ferdig med testinga:
`docker-compose down -v`
(-v sletter volumes)

Om du ikkje gjer dette risikerer du tr√∏bbel neste gong. Det betyr ogs√•: om ting ikkje funker neste gong - start med `docker-compose down` i alle repo ;)

#### Tving nedlastning og bygging av nye images

For √• tvinge nedlastning av nye images (feks hvis det har kommet ett nytt backend image siden sist kj√∏ring) kan man kj√∏re:

`docker-compose down && docker-compose build --pull`

Nokre gonger vil ikkje lydia-api-imaget oppdatere seg. Dette kan du sjekke ved √• gjere `docker images` og sjekke opprettingsdatoen for ghcr.io/navikt/lydia-api. For √• slette imaget gjer du `docker rmi <IMAGE ID>`. IMAGE ID finn du i tabellen fr√• `docker images`.


## Storybook
For √• raskt kunne teste at ein komponent ser ut som den skal har vi laga stories i Storybook. 
Dette gjer det mogleg √• sj√• komponenten i ein n√∏ytral eller bestemt kontekst.

For √• k√∏yre opp dette gjer du  
`npm run storybook` i `./client/`

D√• kan du n√• Storybook i ein nettlesar p√• adressa [localhost:6006](http://localhost:6006).


## Deploy til NAIS  
// TODO skriv noko om 'frackend' og deploy til NAIS  

## Koble til postgresql lokalt via docker-compose oppsett

Sjekk https://github.com/navikt/lydia-api#koble-til-postgresql-lokalt-via-docker-compose-oppsett

---

## Ymse feils√∏king
### Applikasjonen k√∏yrer fint, men etter innlogging f√•r du beskjed om √• logge inn p√• nytt
Dato: 2023-03-23  
Utviklar: Ingrid  
Case:
Frontend og backend k√∏yrer fint. Frontend f√•r opp oAuth. Backend responderer p√• [localhost:8080/internal/isAlive](http://localhost:8080/internal/isalive). Etter innlogging f√•r du feilmeldingsside med beskjed om "trykk her for √• logge inn p√• nytt".

<details>
<summary>
Feils√∏king
</summary>
Sjekk docker logs p√• frackend
`docker ps`
Kopier CONTAINER ID for lydia-radgiver-frontend-frackend 
`docker logs [CONTAINER ID HERE]`
Sjekk om du f√•r feilmeldingar her.

Fordi ein kan (og i tilfelle docker-imaget for frackend er gamalt)
`docker images`, hent ut id for lydia-radgiver-frontend-frackend
`docker rmi [CONTAINER ID HERE]`
Gjer `/run.sh` p√• nytt

Etter dette fungerte ting p√• magisk vis 2023-03-23.

<br>

Andre ting vi pr√∏vde som kanskje/kanskje ikkje hjalp
- installere dependencies i /server
- k√∏yre `npm run dev` i /server (etter at docker-containarar var stoppa) for √• sj√• feilmeldinger litt betre
- docker logs

</details>


###  `[vite] http proxy error at /innloggetAnsatt` + feilmelding om allokert port
Dato: 2023-06-19  
Utviklar: Ingrid og Thomas  
Case:  
F√•r til √• k√∏yre opp frontend med /run.sh, men etter innlogging i OAuth f√•r vi feilmelding i frontend og i terminalen.

Frontend:
> Noe gikk feil ved innlasting av siden.  
> Du kan pr√∏ve √• logge inn p√• nytt ved √• trykke p√• denne lenken.

Terminal:
```bash
10:22:57 AM [vite] http proxy error at /innloggetAnsatt:
Error: connect ECONNREFUSED ::1:3000
    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1494:16)
```

Ved ./run.sh fekk Thomas ein ei feilmelding om at ein port allereie var allokkert. Dette er truleg rota til problemet.

<details>
<summary>
Feils√∏king
</summary>

- Ta ned alle containarar og volumes: `docker-compose down --remove-orphans -v`  
- Fjern dockar-containarar. Ingrid fjerna lydia-api + lydia-radgiver-frontend-frackend,  Thomas fjerna alle. √Ö fjerne alle tek litt lengre tid √• k√∏yre opp, men d√• funka localhost:2222 med ein gong etterp√•, hos Ingrid funka ting etter at ho hadde hatt lunsj.  
- `./run.sh -i` (eller `./run.sh -cfi` om du vil gjere dei to stega over ein ekstra gong)  
- üéâüéâüéâ  

</details>



### `[vite] http proxy error at /innloggetAnsatt` (aka "den gongen vi ikkje googla feilmeldinga")
Dato: 2023-06-20  
Utviklar: Ingrid, (Christian og Per-Christian er med p√• feils√∏king)  

Case:  
F√•r til √• k√∏yre opp frontend med /run.sh, men etter innlogging i OAuth f√•r vi feilmelding i frontend og i terminalen.

Frontend:  
> Noe gikk feil ved innlasting av siden.   
> Du kan pr√∏ve √• logge inn p√• nytt ved √• trykke p√• denne lenken.

Terminal:  
```bash
10:22:57 AM [vite] http proxy error at /innloggetAnsatt:
Error: connect ECONNREFUSED ::1:3000
    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1494:16)
```

F√•r ikkje feilmeldingar ved k√∏yring av ./run.sh f√∏r vi kallar ting fr√• frontend.

<details>
<summary>
Feils√∏king
</summary>

Vi pr√∏vde mykje greier som vi skildrar lengre nede, men fann til slutt problemet ved √• google feilmeldinga fr√• terminalen.  
Dette er artiklane vi fann som forklarte problemet v√•rt:  
- https://github.com/lima-vm/lima/issues/1330  
- https://github.com/nodejs/node/issues/40702  

#### Feilen, kort oppsummert:  
Fr√• Node v17 vert ikkje IP-adresser lengre sortert med IPv4 fyrst. Dette gjer at datamaskina ikkje n√∏dvendigvis finn localhost 127.0.0.1 (IPv4) f√∏r localhost ::1 (IPv6). Lima, som Colima er bygga p√•, st√∏ttar ikkje IPv6 enno. Det betyr:   
N√•r Colima f√•r ::1 som localhost klikkar ting.

#### L√∏ysing:
Hardkode `127.0.0.1` som localhost-adresse i `vite.config.ts` i staden for √• berre skrive `localhost`.



Vi legg med ei oppsummering av ting vi pr√∏vde f√∏r vi googla som ikkje fungerte, som ei p√•minning om √• sp√∏rje internett f√∏r du tenker sj√∏lv i fire timar.

#### Feils√∏king som ikkje funka

- Ta ned alle containarar og volumes: `docker-compose down --remove-orphans -v`  
- K√∏yr opp med `docker-compose up` i root og `npm run dev` i /client for meir gjennomsiktig logging.
- F√•r feilmeldingar om "proxy error at /innloggetAnsatt". At noko skjer p√• :3000 tyder p√• at vi ikkje n√•r frackend.
- (Ein gong rundt her lurer Per-Christian p√• om IPv6 kan vere problemet, vi burde fylgd dette sporet allereie no.)
- Sjekkar logs p√• frackend-container: `docker logs [container id]`. Dei er normale (typ 10-ish linjer)
- Sjekkar logs p√• lydia-api, f√•r masse vanleg r√¶l. 
- Sjekkar isalive p√• dei ulike portane: http://localhost:3000/internal/isalive (frackend), http://localhost:8080/internal/isalive (backend)

No veit vi:  
- wonderwall er oppe (fordi vi f√•r innloggingsprompt og svar p√• 2222)
- frontend er oppe (fordi vi kan sj√• feilmelding i nettlesaren)
- frackend er oppe (isalive 3000)
- backend er oppe (isalive 8080)  

Meir feils√∏king:  
- inspiserar request i Networks i devtools i nettlesar. F√•r "500 internal server error" p√• /innloggetAnsatt.
- K√∏yrar `./run.sh -cif`. F√•r `psql:/tmp/db_script.sql:4606: ERROR:  role "cloudsqliamuser" does not exist` i tillegg til den vanlege `role "testuser" does not exist`. Framleis feil i innlogging. Vi trur vi f√•r denne fordi vi sletta volumes i -c-steget i run.sh

Vi byrjar √• bli svoltne, s√• d√• pr√∏ver vi drastiske ting.   
- Fjerne alle "dangeling" images: `dc down`, s√• `docker image prune`. [Info om kva image prune gjer.](https://docs.docker.com/engine/reference/commandline/image_prune/) Dette fjerna tydelegvis 3-ish greier, mellom anna containaren "none". 
- Fjernar resten av images: `dc down`, s√• `docker image prune -a`. Fjernar dangling images + alle utan minst ein container knytt til seg. Output: `Total reclaimed space: 5.966GB`.
- `docker images` for √• sj√• om alt er borte. 
- `./run.sh` p√• nytt medan vi et lunsj. Dette hjalp heller ikkje. Kult. Vi har s√•nn 7 docker-images no.
-Vi pr√∏ver `docker-network prune`. `docker network ls`  listar nettverk. Vi hadde 3 stk. Etter `docker network prune` har vi framleis 3 stk. 
- Vi stoppar Alt: `docker-compose down`, s√• `docker system prune -a`. Dette fjernar images, alle stoppa containarar, networks og volumes. `Total reclaimed space: 7.259GB`. Kult.
- Vi restartar terminalen, i tilfelle det hjelp p√• noko. `docker-compose down` fyrst.
- Googlar feilmelding: https://github.com/nodejs/node/issues/40702. Finn ut kva problemet var. Tek ein oppgitt pause.
- Bytta ut localhost i vite.config.ts med 127.0.0.1. D√• funka ting etter restart av run.sh.
- üéâüéâüéâ

#### L√¶ringspunkt:
- √Ö google ting burde ikkje vere steg 16, men kanskje s√•nn mellom 1 og 3 ein stad.
- Lytt til Erfarne Fjellfolk n√•r dei nevnar IPv6.
- Om ei adresse ser litt rar ut ‚Äì s√∏k den opp med ein gong. ::1:3000 var jo litt rart, og ville nok leidd oss p√• rett veg.
- N√•r feilmeldinga i terminal seier noko om TCP har kanskje feilen noko med nettverk √• gjere.
- Det er fint √• notere feils√∏kingssteg, d√• har vi betre oversikt over kva vi har gjort.
- Guide fr√• tidlegare buggar var nyttig i √• finne ein stad √• starte feils√∏kinga, sj√∏lv om vi ikkje burde starta der.

</details>


---

### Ukjent feil: Fetch not defined
Dato: 2023-06-20  
Utvikler: Per-Christian  
Case: f√•r feilmelding `Ukjent feil: Fetch not defined`

<details>
<summary>
Oppdater til node 18
</summary>
Om du ikke har nvm sjekk ut [denne linken](https://medium.com/devops-techable/how-to-install-nvm-node-version-manager-on-macos-with-homebrew-1bc10626181) f√∏rst.

```bash
nvm install 18 --latest-npm
nvm alias default 18
nvm use 18

#evt:
nvm install 18 --default --latest-npm
nvm use 18

#sjekk med:
node -v
```
Kj√∏r npm install p√• nytt i b√•de server og client
```bash
cd client
npm install
cd ../server
npm install
```
Stop docker images og slett frackend imaget
```bash
docker-compose down
docker images | grep frackend
docker rmi [image id]
run.sh -cfi
```
* Profit!

</details>


---

### Error occurred while trying to proxy: 127.0.0.1:3000/sykefraversstatistikk/antallTreff?

Dato: 2023-08-07
Utviklar: Ingrid

Case:
F√•r timeout p√• api-kall og beskjed om feil i tilkopling til proxy. 
Det er fyrste dag etter sommarferien, backend hadde 44 commits, frontend 4, sidan sist eg pulla.

<details>
<summary> Feils√∏king </summary>@

#### Problemet  
Det er sannsynlegvis lenge sidan sist nokon pr√∏vde √• k√∏yre opp Fia lokalt fr√• frontend, s√• eg mistenker feilen handlar om ei endring i backend som frontend ikkje har f√•tt med seg.


- f√•r ikkje tak i filterverdiar fordi timeout(504): GET
  http://localhost:2222/api/sykefraversstatistikk/filterverdier  
- ogs√• timeout p√• http://localhost:2222/api/sykefraversstatistikk/publiseringsinfo  
- Har ogs√• (forh√•pentlegvis ikkje relaterte) feilmeldingar om at sida ikkje liker √• skulle vise feilmelding-banner samstundes som den skal vise "ny statistikk publiseres"-banner  
- Andre timeouts: GET
  http://localhost:2222/api/sykefraversstatistikk og GET
  http://localhost:2222/api/sykefraversstatistikk/antallTreff, sistnemnde f√•r feilmeldingbanneret.  

#### Feils√∏king
- ./run.sh -cfi
- Sletta images som er relaterte  
- Dobbeltsjekka at vi bruker same $DB_DUMP i backend og frontend
- Sjekka for feilmeldingar etter ./run.sh  
- K√∏yrer `pus` i terminal (`docker ps`), ser at lydia-api st√•r som "restarting". Det pleier sjeldan vere bra.
- (Har ogs√• "ReferenceError: utilsService is not defined" i frontend-konsollen)
- Er p√• naisdevice, har oppdatert brew, har pulla nyaste frontend (og backend)

- K√∏yrer opp med `dc up` og `npm run dev` i client. F√•r d√• feilmelding i terminal for docker: `lydia-radgiver-frontend-frackend-1    | [HPM] Error occurred while proxying request 127.0.0.1:3000/sykefraversstatistikk/publiseringsinfo to http://backend:8080/ [ENOTFOUND] (https://nodejs.org/api/errors.html#errors_common_system_errors)
  `
- S√• kom det (kanskje?) gull:
```
lydia-radgiver-frontend-backend-1 exited with code 1
lydia-radgiver-frontend-backend-1     | Picked up JAVA_TOOL_OPTIONS: -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005
lydia-radgiver-frontend-backend-1     | Listening for transport dt_socket at address: 5005
lydia-radgiver-frontend-backend-1     | Exception in thread "main" 
lydia-radgiver-frontend-backend-1     | java.lang.RuntimeException: Missing required variable STATISTIKK_METADATA_VIRKSOMHET_TOPIC
lydia-radgiver-frontend-backend-1     | 	at no.nav.lydia.NaisEnvironmentKt.getEnvVar(NaisEnvironment.kt:155)
lydia-radgiver-frontend-backend-1     | 	at no.nav.lydia.NaisEnvironmentKt.getEnvVar$default(NaisEnvironment.kt:154)
lydia-radgiver-frontend-backend-1     | 	at no.nav.lydia.Kafka.<init>(NaisEnvironment.kt:83)
lydia-radgiver-frontend-backend-1     | 	at no.nav.lydia.NaisEnvironment.<init>(NaisEnvironment.kt:14)
lydia-radgiver-frontend-backend-1     | 	at no.nav.lydia.AppKt.startLydiaBackend(App.kt:69)
lydia-radgiver-frontend-backend-1     | 	at no.nav.lydia.AppKt.main(App.kt:65)
lydia-radgiver-frontend-backend-1     | 	at no.nav.lydia.AppKt.main(App.kt)
lydia-radgiver-frontend-backend-1 exited with code 1
```

Legg til `STATISTIKK_METADATA_VIRKSOMHET_TOPIC` i `docker-compose.yaml`, hentar verdiar fr√• backend. K√∏yrer opp p√• nytt med `./run.sh -cfi`

Denne gongen f√•r eg feilmelding om `STATISTIKK_SEKTOR_TOPIC` etter √• ha venta litt. Eg trur eg ser m√∏nsteret.

</details>

L√∏ysing:
Leggje inn alle manglande kafka-topics i `docker-compose.yaml`. Hentar verdiar fr√• `nais.yaml` i backend (lydia-api). 

L√¶rdom:
- Framleis fint √• k√∏yre opp ting med `dc up` + `npm run dev`, d√• f√•r ein betre feilmeldingar.
- Det er lurt √• la ting k√∏yre ein stund etter at du har framprovosert feilen, i tilfelle terminalen spyttar ut fleire feilmeldingar etter kvart. Det gjorde den i dag. Det viste seg at om ein venta nokre minutt spytta den ut alle manglande topics.  

---

### Klarer ikke √• laste ned backend fra ghcr.io
Fikk authentication feil ven nedlasting... Kj√∏rte ```docker logout ghcr.io``` og ble glad igjen.
Litt usikker p√• hva som skjedde.


# Henvendelser

Sp√∏rsm√•l knyttet til koden eller prosjektet kan stilles som issues her p√• GitHub

## For NAV-ansatte

Interne henvendelser kan sendes via Slack i kanalen [#team-pia-utvikling](https://nav-it.slack.com/archives/C02T6RG9AE4).

# lydia-radgiver-frontend

Frontend for IA r√•dgivere

# Komme i gang

## Kj√∏re opp lokalt TLDR;

`./run.sh [-if]` i rot mappe

## Kj√∏re opp lokalt

Dette let deg k√∏yre opp frontend lokalt med alt av avhengigheitene som trengst.
Dette vert orkestrert av docker-compose.

### Avhengigheiter som vert k√∏yrd

- backend: lydia-api
- postgres
- kafka
- wonderwall
- mockOAuth2-server

### Tilgang til npm pakker p√• github

Lag token p√• github med packages:read rettigheter.
Legg s√• dette inn i ~/.npmrc slik som dette:
`//npm.pkg.github.com/:_authToken=<generert token med packages:read rettigheter>`
Husk √• logge p√• gcp med `gcloud auth login --update-adc`

### F√∏r du startar

Sjekk om du har postgresql-klient ved √• k√∏yre
`psql --version`

Installer ein postgresql-klient, for eksempel ved √• gjere  
`brew install libpq`  
Legg s√• til psql i PATH. Du vil f√• instruksjonar p√• dette fr√• brew, og det ser litt s√•nn ut:

> If you need to have libpq first in your PATH, run:  
> echo 'export PATH="/opt/homebrew/opt/libpq/bin:$PATH"' >> ~/.zshrc

Sjekk at du har colima `colima version`  
Om du ikkje har colima, k√∏yr `brew install colima`

Sjekk at du har docker `docker -v`  
Om du ikkje har docker, k√∏yr `brew install docker`

Vi skal leggje til linja `127.0.0.1 host.docker.internal` i fila `/etc/hosts` i rota av datamaskina di.
Da vil nettlesaren automatisk fange opp wonderwall og mock-oauth2-server, som da vil resolve det til localhost.

Dette kan du til d√∏mes gjere ved √• skrive `sudo vi /etc/hosts` i terminalen og s√•
leggje til `127.0.0.1 host.docker.internal` nedst p√• ei ega linje.
S√∏rg for at du har skrudd p√• administrator-rettar for brukaren din (privileges.app p√• mac).

#### Tilgang til backend docker image

`gcloud auth configure-docker europe-north1-docker.pkg.dev`  
`gcloud auth login --update-adc`  
`gcloud components install docker-credential-gcr`

### K√∏yr server og frontend med avhengigheter:

Pass p√• at du har ein k√∏yrande docker-instans.
For Colima gjer du dette ved √• k√∏yre `colima status` og eventuelt `colima start`

Om du bruker Colima treng du ogs√• √• k√∏yre denne for √• f√• ting til √• fungere. Hugs √• gjer brukaren din admin-rettar f√∏rst.  
`sudo rm -rf /var/run/docker.sock && sudo ln -s /Users/$(whoami)/.colima/docker.sock /var/run/docker.sock
`

N√•r du har docker p√• plass kan du k√∏yre  
`./run.sh -if` i rotmappa. Dette startar server og frontend med sine avhengigheiter.

Bes√∏k deretter http://localhost:2222 i din favorittbrowser.  
Skjemaet som dukker opp her gjev deg h√∏ve til √• endre kva rettar testbrukaren din har. I dei fleste tilfelle kan du skrive inn noko tilfeldig tekst og trykke "SIGN-IN".

### Rydd opp etter deg n√•r du er ferdig med testinga:

`docker-compose down -v`
(-v sletter volumes)

Om du ikkje gjer dette risikerer du tr√∏bbel neste gong. Det betyr ogs√•: om ting ikkje funker neste gong - start med `docker-compose down` i alle repo ;)

#### Tving nedlastning og bygging av nye images

For √• tvinge nedlastning av nye images (feks hvis det har kommet ett nytt backend image siden sist kj√∏ring) kan man kj√∏re:

`docker-compose down && docker-compose build --pull`

Nokre gonger vil ikkje lydia-api-imaget oppdatere seg. Dette kan du sjekke ved √• gjere `docker images` og sjekke opprettingsdatoen for ghcr.io/navikt/lydia-api. For √• slette imaget gjer du `docker rmi <IMAGE ID>`. IMAGE ID finn du i tabellen fr√• `docker images`.

### K√∏yre frontend lokalt med lokal backend

1. √Öpne terminalvindu i `lydia-api`, s√∏rg for at du har sjekket ut branchen du vil bygge.
2. Kj√∏r `./gradlew clean build -x test` (`-x test` gjer at vi slepp √• vente p√• at alle testane skal k√∏yre)
3. G√• til `lydia-radgiver-frontend` og √•pne filen som heter `docker-compose.yaml`. 
4. Finn linjen:
```
backend:
    image: europe-north1-docker.pkg.dev/nais-management-233d/pia/lydia-api:latest
```
5. Bytt den ut med dette (sjekk gjerne at det er rett path til `lydia-api` prosjektet ditt):
```
backend:
    build:
        context: ../lydia-api
```
6. √Öpne terminalvindu i `lydia-radgiver-frontend` og kj√∏r `docker images`
7. Finn og kopier image id for `lydia-radgiver-frontend-backend` og eventuelt `lydia-api-backend`
8. Fjern disse imagene ved √• kj√∏re `docker rmi <kopiert image id> `
9. K√∏yr `./run.sh` fra `lydia-radgiver-frontend` som vanlig
10. Profit üéâüéâüéâ

Hugs √• ikkje committe endringa du har gjort i `docker-compose.yaml`.

## Deploy til NAIS

// TODO skriv noko om 'frackend' og deploy til NAIS

## Koble til postgresql lokalt via docker-compose oppsett

Sjekk https://github.com/navikt/lydia-api#koble-til-postgresql-lokalt-via-docker-compose-oppsett

---

## Ymse feils√∏king

### Applikasjonen k√∏yrer fint, men etter innlogging f√•r du beskjed om √• logge inn p√• nytt

Dato: 2023-03-23  
Utviklar: Ingrid  
Case:
Frontend og backend k√∏yrer fint. Frontend f√•r opp oAuth. Backend responderer p√• [localhost:8080/internal/isAlive](http://localhost:8080/internal/isalive). Etter innlogging f√•r du feilmeldingsside med beskjed om "trykk her for √• logge inn p√• nytt".

<details>
<summary>
Feils√∏king
</summary>
Sjekk docker logs p√• frackend
`docker ps`
Kopier CONTAINER ID for lydia-radgiver-frontend-frackend 
`docker logs [CONTAINER ID HERE]`
Sjekk om du f√•r feilmeldingar her.

Fordi ein kan (og i tilfelle docker-imaget for frackend er gamalt)
`docker images`, hent ut id for lydia-radgiver-frontend-frackend
`docker rmi [CONTAINER ID HERE]`
Gjer `/run.sh` p√• nytt

Etter dette fungerte ting p√• magisk vis 2023-03-23.

<br>

Andre ting vi pr√∏vde som kanskje/kanskje ikkje hjalp

- installere dependencies i /server
- k√∏yre `npm run dev` i /server (etter at docker-containarar var stoppa) for √• sj√• feilmeldinger litt betre
- docker logs

</details>

### `[vite] http proxy error at /innloggetAnsatt` + feilmelding om allokert port

Dato: 2023-06-19  
Utviklar: Ingrid og Thomas  
Case:  
F√•r til √• k√∏yre opp frontend med /run.sh, men etter innlogging i OAuth f√•r vi feilmelding i frontend og i terminalen.

Frontend:

> Noe gikk feil ved innlasting av siden.  
> Du kan pr√∏ve √• logge inn p√• nytt ved √• trykke p√• denne lenken.

Terminal:

```bash
10:22:57 AM [vite] http proxy error at /innloggetAnsatt:
Error: connect ECONNREFUSED ::1:3000
    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1494:16)
```

Ved ./run.sh fekk Thomas ein ei feilmelding om at ein port allereie var allokkert. Dette er truleg rota til problemet.

<details>
<summary>
Feils√∏king
</summary>

- Ta ned alle containarar og volumes: `docker-compose down --remove-orphans -v`
- Fjern dockar-containarar. Ingrid fjerna lydia-api + lydia-radgiver-frontend-frackend, Thomas fjerna alle. √Ö fjerne alle tek litt lengre tid √• k√∏yre opp, men d√• funka localhost:2222 med ein gong etterp√•, hos Ingrid funka ting etter at ho hadde hatt lunsj.
- `./run.sh -i` (eller `./run.sh -cfi` om du vil gjere dei to stega over ein ekstra gong)
- üéâüéâüéâ

</details>

### `[vite] http proxy error at /innloggetAnsatt` (aka "den gongen vi ikkje googla feilmeldinga")

Dato: 2023-06-20  
Utviklar: Ingrid, (Christian og Per-Christian er med p√• feils√∏king)

Case:  
F√•r til √• k√∏yre opp frontend med /run.sh, men etter innlogging i OAuth f√•r vi feilmelding i frontend og i terminalen.

Frontend:

> Noe gikk feil ved innlasting av siden.  
> Du kan pr√∏ve √• logge inn p√• nytt ved √• trykke p√• denne lenken.

Terminal:

```bash
10:22:57 AM [vite] http proxy error at /innloggetAnsatt:
Error: connect ECONNREFUSED ::1:3000
    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1494:16)
```

F√•r ikkje feilmeldingar ved k√∏yring av ./run.sh f√∏r vi kallar ting fr√• frontend.

#### Feilen, kort oppsummert:

Fr√• Node v17 vert ikkje IP-adresser lengre sortert med IPv4 fyrst. Dette gjer at datamaskina ikkje n√∏dvendigvis finn localhost 127.0.0.1 (IPv4) f√∏r localhost ::1 (IPv6). Lima, som Colima er bygga p√•, st√∏ttar ikkje IPv6 enno. Det betyr:  
N√•r Colima f√•r ::1 som localhost klikkar ting.

#### L√∏ysing:

Hardkode `127.0.0.1` som localhost-adresse i `vite.config.ts` i staden for √• berre skrive `localhost`.

<details>
<summary>
Feils√∏king
</summary>

Vi pr√∏vde mykje greier som vi skildrar lengre nede, men fann til slutt problemet ved √• google feilmeldinga fr√• terminalen.  
Dette er artiklane vi fann som forklarte problemet v√•rt:

- https://github.com/lima-vm/lima/issues/1330
- https://github.com/nodejs/node/issues/40702

Vi legg med ei oppsummering av ting vi pr√∏vde f√∏r vi googla som ikkje fungerte, som ei p√•minning om √• sp√∏rje internett f√∏r du tenker sj√∏lv i fire timar.

#### Feils√∏king som ikkje funka

- Ta ned alle containarar og volumes: `docker-compose down --remove-orphans -v`
- K√∏yr opp med `docker-compose up` i root og `npm run dev` i /client for meir gjennomsiktig logging.
- F√•r feilmeldingar om "proxy error at /innloggetAnsatt". At noko skjer p√• :3000 tyder p√• at vi ikkje n√•r frackend.
- (Ein gong rundt her lurer Per-Christian p√• om IPv6 kan vere problemet, vi burde fylgd dette sporet allereie no.)
- Sjekkar logs p√• frackend-container: `docker logs [container id]`. Dei er normale (typ 10-ish linjer)
- Sjekkar logs p√• lydia-api, f√•r masse vanleg r√¶l.
- Sjekkar isalive p√• dei ulike portane: http://localhost:3000/internal/isalive (frackend), http://localhost:8080/internal/isalive (backend)

No veit vi:

- wonderwall er oppe (fordi vi f√•r innloggingsprompt og svar p√• 2222)
- frontend er oppe (fordi vi kan sj√• feilmelding i nettlesaren)
- frackend er oppe (isalive 3000)
- backend er oppe (isalive 8080)

Meir feils√∏king:

- inspiserar request i Networks i devtools i nettlesar. F√•r "500 internal server error" p√• /innloggetAnsatt.
- K√∏yrar `./run.sh -cif`. F√•r `psql:/tmp/db_script.sql:4606: ERROR:  role "cloudsqliamuser" does not exist` i tillegg til den vanlege `role "testuser" does not exist`. Framleis feil i innlogging. Vi trur vi f√•r denne fordi vi sletta volumes i -c-steget i run.sh

Vi byrjar √• bli svoltne, s√• d√• pr√∏ver vi drastiske ting.

- Fjerne alle "dangeling" images: `dc down`, s√• `docker image prune`. [Info om kva image prune gjer.](https://docs.docker.com/engine/reference/commandline/image_prune/) Dette fjerna tydelegvis 3-ish greier, mellom anna containaren "none".
- Fjernar resten av images: `dc down`, s√• `docker image prune -a`. Fjernar dangling images + alle utan minst ein container knytt til seg. Output: `Total reclaimed space: 5.966GB`.
- `docker images` for √• sj√• om alt er borte.
- `./run.sh` p√• nytt medan vi et lunsj. Dette hjalp heller ikkje. Kult. Vi har s√•nn 7 docker-images no.
  -Vi pr√∏ver `docker-network prune`. `docker network ls` listar nettverk. Vi hadde 3 stk. Etter `docker network prune` har vi framleis 3 stk.
- Vi stoppar Alt: `docker-compose down`, s√• `docker system prune -a`. Dette fjernar images, alle stoppa containarar, networks og volumes. `Total reclaimed space: 7.259GB`. Kult.
- Vi restartar terminalen, i tilfelle det hjelp p√• noko. `docker-compose down` fyrst.
- Googlar feilmelding: https://github.com/nodejs/node/issues/40702. Finn ut kva problemet var. Tek ein oppgitt pause.
- Bytta ut localhost i vite.config.ts med 127.0.0.1. D√• funka ting etter restart av run.sh.
- üéâüéâüéâ

</details>

#### L√¶ringspunkt:

- √Ö google ting burde ikkje vere steg 16, men kanskje s√•nn mellom 1 og 3 ein stad.
- Lytt til Erfarne Fjellfolk n√•r dei nevnar IPv6.
- Om ei adresse ser litt rar ut ‚Äì s√∏k den opp med ein gong. ::1:3000 var jo litt rart, og ville nok leidd oss p√• rett veg.
- N√•r feilmeldinga i terminal seier noko om TCP har kanskje feilen noko med nettverk √• gjere.
- Det er fint √• notere feils√∏kingssteg, d√• har vi betre oversikt over kva vi har gjort.
- Guide fr√• tidlegare buggar var nyttig i √• finne ein stad √• starte feils√∏kinga, sj√∏lv om vi ikkje burde starta der.

---

### Ukjent feil: Fetch not defined

Dato: 2023-06-20  
Utvikler: Per-Christian  
Case: f√•r feilmelding `Ukjent feil: Fetch not defined`

<details>
<summary>
Oppdater til node 18
</summary>
Om du ikke har nvm sjekk ut [denne linken](https://medium.com/devops-techable/how-to-install-nvm-node-version-manager-on-macos-with-homebrew-1bc10626181) f√∏rst.

```bash
nvm install 18 --latest-npm
nvm alias default 18
nvm use 18

#evt:
nvm install 18 --default --latest-npm
nvm use 18

#sjekk med:
node -v
```

Kj√∏r npm install p√• nytt i b√•de server og client

```bash
cd client
npm install
cd ../server
npm install
```

Stop docker images og slett frackend imaget

```bash
docker-compose down
docker images | grep frackend
docker rmi [image id]
run.sh -cfi
```

- Profit!

</details>

---

### Error occurred while trying to proxy: 127.0.0.1:3000/sykefraversstatistikk/antallTreff?

Dato: 2023-08-07
Utviklar: Ingrid

Case:
F√•r timeout p√• api-kall og beskjed om feil i tilkopling til proxy.
Det er fyrste dag etter sommarferien, backend hadde 44 commits, frontend 4, sidan sist eg pulla.

<details>
<summary> Feils√∏king </summary>

#### Problemet

Det er sannsynlegvis lenge sidan sist nokon pr√∏vde √• k√∏yre opp Fia lokalt fr√• frontend, s√• eg mistenker feilen handlar om ei endring i backend som frontend ikkje har f√•tt med seg.

- f√•r ikkje tak i filterverdiar fordi timeout(504): GET
  http://localhost:2222/api/sykefraversstatistikk/filterverdier
- ogs√• timeout p√• http://localhost:2222/api/sykefraversstatistikk/publiseringsinfo
- Har ogs√• (forh√•pentlegvis ikkje relaterte) feilmeldingar om at sida ikkje liker √• skulle vise feilmelding-banner samstundes som den skal vise "ny statistikk publiseres"-banner
- Andre timeouts: GET
  http://localhost:2222/api/sykefraversstatistikk og GET
  http://localhost:2222/api/sykefraversstatistikk/antallTreff, sistnemnde f√•r feilmeldingbanneret.

#### Feils√∏king

- ./run.sh -cfi
- Sletta images som er relaterte
- Dobbeltsjekka at vi bruker same $DB_DUMP i backend og frontend
- Sjekka for feilmeldingar etter ./run.sh
- K√∏yrer `pus` i terminal (`docker ps`), ser at lydia-api st√•r som "restarting". Det pleier sjeldan vere bra.
- (Har ogs√• "ReferenceError: utilsService is not defined" i frontend-konsollen)
- Er p√• naisdevice, har oppdatert brew, har pulla nyaste frontend (og backend)

- K√∏yrer opp med `dc up` og `npm run dev` i client. F√•r d√• feilmelding i terminal for docker: `lydia-radgiver-frontend-frackend-1    | [HPM] Error occurred while proxying request 127.0.0.1:3000/sykefraversstatistikk/publiseringsinfo to http://backend:8080/ [ENOTFOUND] (https://nodejs.org/api/errors.html#errors_common_system_errors)
`
- S√• kom det (kanskje?) gull:

```
lydia-radgiver-frontend-backend-1 exited with code 1
lydia-radgiver-frontend-backend-1     | Picked up JAVA_TOOL_OPTIONS: -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005
lydia-radgiver-frontend-backend-1     | Listening for transport dt_socket at address: 5005
lydia-radgiver-frontend-backend-1     | Exception in thread "main"
lydia-radgiver-frontend-backend-1     | java.lang.RuntimeException: Missing required variable STATISTIKK_METADATA_VIRKSOMHET_TOPIC
lydia-radgiver-frontend-backend-1     | 	at no.nav.lydia.NaisEnvironmentKt.getEnvVar(NaisEnvironment.kt:155)
lydia-radgiver-frontend-backend-1     | 	at no.nav.lydia.NaisEnvironmentKt.getEnvVar$default(NaisEnvironment.kt:154)
lydia-radgiver-frontend-backend-1     | 	at no.nav.lydia.Kafka.<init>(NaisEnvironment.kt:83)
lydia-radgiver-frontend-backend-1     | 	at no.nav.lydia.NaisEnvironment.<init>(NaisEnvironment.kt:14)
lydia-radgiver-frontend-backend-1     | 	at no.nav.lydia.AppKt.startLydiaBackend(App.kt:69)
lydia-radgiver-frontend-backend-1     | 	at no.nav.lydia.AppKt.main(App.kt:65)
lydia-radgiver-frontend-backend-1     | 	at no.nav.lydia.AppKt.main(App.kt)
lydia-radgiver-frontend-backend-1 exited with code 1
```

Legg til `STATISTIKK_METADATA_VIRKSOMHET_TOPIC` i `docker-compose.yaml`, hentar verdiar fr√• backend. K√∏yrer opp p√• nytt med `./run.sh -cfi`

Denne gongen f√•r eg feilmelding om `STATISTIKK_SEKTOR_TOPIC` etter √• ha venta litt. Eg trur eg ser m√∏nsteret.

</details>

L√∏ysing:
Leggje inn alle manglande kafka-topics i `docker-compose.yaml`. Hentar verdiar fr√• `nais.yaml` i backend (lydia-api).

L√¶rdom:

- Framleis fint √• k√∏yre opp ting med `dc up` + `npm run dev`, d√• f√•r ein betre feilmeldingar.
- Det er lurt √• la ting k√∏yre ein stund etter at du har framprovosert feilen, i tilfelle terminalen spyttar ut fleire feilmeldingar etter kvart. Det gjorde den i dag. Det viste seg at om ein venta nokre minutt spytta den ut alle manglande topics.

### Socket-hangup (den som liknar p√• localhost-ipv6-buggen)

Dato: 2023-09-14  
Utviklar: Ingrid og Christian og Nima (men Thomas har ogs√• hatt problemet.)

Case: F√•r k√∏yrd opp ting, men f√•r

> Noe gikk feil ved innlasting av siden.
> Du kan pr√∏ve √• logge inn p√• nytt ved √• trykke p√• denne lenken.

og dei to feilmeldingene du ser i "details"-blokker under her.

<details>
<summary>
Feilmelding fr√• `./run.sh`:
</summary>

```bash
10:22:57 AM [vite] http proxy error at /innloggetAnsatt:
Error: socket hang up
at connResetException (node:internal/errors:717:14)
at Socket.socketOnEnd (node:_http_client:526:23)
at Socket.emit (node:events:525:35)
at endReadableNT (node:internal/streams/readable:1359:12)
at process.processTicksAndRejections (node:internal/process/task_queues:82:21)
```

</details>

<details>
<summary>
Feilmelding fr√• `docker logs [frackend-container-id]`
</summary>

```bash
[nodemon] watching path(s): *.*
[nodemon] watching extensions: ts,json
[nodemon] starting `ts-node server.ts`
/home/node/app/node_modules/ts-node/src/index.ts:859
    return new TSError(diagnosticText, diagnosticCodes, diagnostics);
           ^
TSError: ‚®Ø Unable to compile TypeScript:
app.ts(78,64): error TS2554: Expected 2 arguments, but got 3.

    at createTSError (/home/node/app/node_modules/ts-node/src/index.ts:859:12)
    at reportTSError (/home/node/app/node_modules/ts-node/src/index.ts:863:19)
    at getOutput (/home/node/app/node_modules/ts-node/src/index.ts:1077:36)
    at Object.compile (/home/node/app/node_modules/ts-node/src/index.ts:1433:41)
    at Module.m._compile (/home/node/app/node_modules/ts-node/src/index.ts:1617:30)
    at Module._extensions..js (node:internal/modules/cjs/loader:1310:10)
    at Object.require.extensions.<computed> [as .ts] (/home/node/app/node_modules/ts-node/src/index.ts:1621:12)
    at Module.load (node:internal/modules/cjs/loader:1119:32)
    at Function.Module._load (node:internal/modules/cjs/loader:960:12)
    at Module.require (node:internal/modules/cjs/loader:1143:19) {
  diagnosticCodes: [ 2554 ]
}
[nodemon] app crashed - waiting for file changes before starting...
```

</details>

<details>
<summary>
Feils√∏king
</summary>
- `dc down`, s√• `colima stop`. Start colima att og k√∏yr `/run.sh`. Dette + litt venting l√∏yste det hos Thomas.
- `brew update` og `brew upgrade`
- K√∏yrer opp `docker compose up` og `npm run dev` kvar for seg for √• kunne sj√• fleire loggar. Ser same feil, no tydelegare at den er sendt fr√• frontend. F√•r ikkje noko vettugt fr√• `docker logs [lydia-api-id]`
- pr√∏var √• k√∏yre opp med `./run.sh -cfi`
- Pullar nyaste endringar fr√• git. F√•r same feil.
- Slettar frackend-, wonderwall- og backend-imaget. `./run.sh`. D√• funka det hos Ingrid etterp√•, men ikkje hos Christian.
- Etter litt andre random steg fungerer det hos Christian ogs√•.
- Slett absolutt alt: `dc down`, s√• `docker system prune -a`. D√• fungerte det hos Nima ogs√•.

</details>

Konklusjon:  
Vi veit ikkje heilt kva som var gale. Kanskje frackend, kanskje wonderwall. Pr√∏v litt ulike ting, det er v√•rt beste forslag.

---

### Klarer ikke √• laste ned backend fra ghcr.io

Fikk authentication feil ven nedlasting... Kj√∏rte `docker logout ghcr.io` og ble glad igjen.
Litt usikker p√• hva som skjedde.

# Henvendelser

Sp√∏rsm√•l knyttet til koden eller prosjektet kan stilles som issues her p√• GitHub

## For NAV-ansatte

Interne henvendelser kan sendes via Slack i kanalen [#team-pia-utvikling](https://nav-it.slack.com/archives/C02T6RG9AE4).
